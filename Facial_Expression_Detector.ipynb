{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdaa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90009038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Set the path to the directory containing your training and test folders\n",
    "data_directory = \"C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Unaugmented_Images/test\"\n",
    "\n",
    "# Set the path to the output directory where augmented images will be saved\n",
    "output_directory = \"C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Augmented_Images/test\"\n",
    "\n",
    "# Set the desired size for augmented images\n",
    "output_size = (96, 96)  # Change dimensions as needed\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "# Function to convert grayscale image to RGB\n",
    "def convert_to_rgb(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Function to perform image augmentation\n",
    "def augment_image(image, output_path):\n",
    "    # Rotation\n",
    "    angle = np.random.randint(-15, 15)  # Random rotation angle between -15 and 15 degrees\n",
    "    rows, cols = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "    # Translation\n",
    "    x_shift = np.random.randint(-10, 10)  # Random horizontal shift between -10 and 10 pixels\n",
    "    y_shift = np.random.randint(-10, 10)  # Random vertical shift between -10 and 10 pixels\n",
    "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
    "    translated_image = cv2.warpAffine(rotated_image, M, (cols, rows))\n",
    "\n",
    "    # Flip\n",
    "    flipped_image = cv2.flip(translated_image, 1)  # Flip horizontally\n",
    "\n",
    "    # Resize to desired output size\n",
    "    resized_image = cv2.resize(flipped_image, output_size)\n",
    "\n",
    "    # Save the augmented image\n",
    "    cv2.imwrite(output_path, convert_to_rgb(resized_image))\n",
    "\n",
    "\n",
    "# Iterate through each folder in the data directory\n",
    "for folder in os.listdir(data_directory):\n",
    "    folder_path = os.path.join(data_directory, folder)\n",
    "    \n",
    "    # Skip non-directory files\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Create a corresponding output folder in the train/test subdirectory of the output directory\n",
    "    output_subfolder = os.path.join(output_directory, folder)\n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.makedirs(output_subfolder)\n",
    "    \n",
    "    # Traverse through each image file in the current folder\n",
    "    for image_file in glob.glob(os.path.join(folder_path, \"*.jpg\")):\n",
    "        # Read the grayscale image\n",
    "        image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Generate a unique output file name for the augmented image\n",
    "        output_file = os.path.basename(image_file)\n",
    "        output_path = os.path.join(output_subfolder, output_file)\n",
    "\n",
    "        # Apply augmentation and save the augmented image\n",
    "        augment_image(image, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ed5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "I augmented images without converting them to RGB from grayscale so I need to now delete those images from different folders.\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set the paths to the output directories for train, test, and validation\n",
    "train_output_directory = \"C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output/train\"\n",
    "test_output_directory = \"C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output/test\"\n",
    "val_output_directory = \"C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output/validation\"\n",
    "\n",
    "# Function to delete all files in a directory\n",
    "def delete_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)\n",
    "\n",
    "# Delete the existing grayscale images in the train output directory\n",
    "for emotion_folder in os.listdir(train_output_directory):\n",
    "    emotion_folder_path = os.path.join(train_output_directory, emotion_folder)\n",
    "    delete_files(emotion_folder_path)\n",
    "    os.rmdir(emotion_folder_path)\n",
    "\n",
    "# Delete the existing grayscale images in the test output directory\n",
    "for emotion_folder in os.listdir(test_output_directory):\n",
    "    emotion_folder_path = os.path.join(test_output_directory, emotion_folder)\n",
    "    delete_files(emotion_folder_path)\n",
    "    os.rmdir(emotion_folder_path)\n",
    "\n",
    "# Delete the existing grayscale images in the validation output directory\n",
    "for emotion_folder in os.listdir(val_output_directory):\n",
    "    emotion_folder_path = os.path.join(val_output_directory, emotion_folder)\n",
    "    delete_files(emotion_folder_path)\n",
    "    os.rmdir(emotion_folder_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c77696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbe212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Augmented_Images/train'\n",
    "test_folder = 'C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Augmented_Images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1422ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output'\n",
    "train_output_folder = os.path.join(output_folder, 'train')\n",
    "test_output_folder = os.path.join(output_folder, 'test')\n",
    "val_output_folder = os.path.join(output_folder, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb76d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(train_output_folder, exist_ok=True)\n",
    "os.makedirs(test_output_folder, exist_ok=True)\n",
    "os.makedirs(val_output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b4809",
   "metadata": {},
   "source": [
    "WOW, I only created a folder \"Output\" and these commands created 3 sub-folders inside it 'train', 'test', 'validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27cce273",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8  # 80% of the data for training\n",
    "test_ratio = 0.1  # 10% of the data for testing\n",
    "val_ratio = 0.1   # 10% of the data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaa72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for emotion_folder in os.listdir(train_folder):\n",
    "    emotion_folder_path = os.path.join(train_folder, emotion_folder)\n",
    "    images = os.listdir(emotion_folder_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_count = int(train_ratio * len(images))\n",
    "    test_count = int(test_ratio * len(images))\n",
    "    val_count = len(images) - train_count - test_count\n",
    "\n",
    "    train_images = images[:train_count]\n",
    "    test_images = images[train_count:train_count + test_count]\n",
    "    val_images = images[train_count + test_count:]\n",
    "\n",
    "    for image in train_images:\n",
    "        src_path = os.path.join(emotion_folder_path, image)\n",
    "        dst_path = os.path.join(train_output_folder, emotion_folder, image)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "    for image in test_images:\n",
    "        src_path = os.path.join(emotion_folder_path, image)\n",
    "        dst_path = os.path.join(test_output_folder, emotion_folder, image)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "    for image in val_images:\n",
    "        src_path = os.path.join(emotion_folder_path, image)\n",
    "        dst_path = os.path.join(val_output_folder, emotion_folder, image)\n",
    "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50cace24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 3, 3, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,801,927\n",
      "Trainable params: 1,214,215\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "Found 22965 images belonging to 7 classes.\n",
      "Found 2876 images belonging to 7 classes.\n",
      "Found 2868 images belonging to 7 classes.\n",
      "Epoch 1/20\n",
      "717/717 [==============================] - 621s 859ms/step - loss: 1.9013 - accuracy: 0.2084 - val_loss: 1.8422 - val_accuracy: 0.2504 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "717/717 [==============================] - 502s 700ms/step - loss: 1.8485 - accuracy: 0.2277 - val_loss: 1.8327 - val_accuracy: 0.2514 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "717/717 [==============================] - 438s 611ms/step - loss: 1.8375 - accuracy: 0.2342 - val_loss: 1.8268 - val_accuracy: 0.2521 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "717/717 [==============================] - 483s 673ms/step - loss: 1.8310 - accuracy: 0.2436 - val_loss: 1.8228 - val_accuracy: 0.2518 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "717/717 [==============================] - 526s 733ms/step - loss: 1.8254 - accuracy: 0.2451 - val_loss: 1.8211 - val_accuracy: 0.2504 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "717/717 [==============================] - 476s 664ms/step - loss: 1.8237 - accuracy: 0.2470 - val_loss: 1.8202 - val_accuracy: 0.2504 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "717/717 [==============================] - 497s 693ms/step - loss: 1.8209 - accuracy: 0.2499 - val_loss: 1.8220 - val_accuracy: 0.2511 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "717/717 [==============================] - 514s 716ms/step - loss: 1.8200 - accuracy: 0.2499 - val_loss: 1.8162 - val_accuracy: 0.2511 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "717/717 [==============================] - 567s 791ms/step - loss: 1.8193 - accuracy: 0.2504 - val_loss: 1.8187 - val_accuracy: 0.2504 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "717/717 [==============================] - 579s 808ms/step - loss: 1.8165 - accuracy: 0.2511 - val_loss: 1.8142 - val_accuracy: 0.2514 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "717/717 [==============================] - 586s 817ms/step - loss: 1.8161 - accuracy: 0.2513 - val_loss: 1.8167 - val_accuracy: 0.2521 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "717/717 [==============================] - 588s 820ms/step - loss: 1.8175 - accuracy: 0.2515 - val_loss: 1.8143 - val_accuracy: 0.2511 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "717/717 [==============================] - 572s 798ms/step - loss: 1.8162 - accuracy: 0.2514 - val_loss: 1.8153 - val_accuracy: 0.2511 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "717/717 [==============================] - 614s 855ms/step - loss: 1.8164 - accuracy: 0.2515 - val_loss: 1.8149 - val_accuracy: 0.2511 - lr: 1.0000e-05\n",
      "Epoch 15/20\n",
      "717/717 [==============================] - 565s 788ms/step - loss: 1.8147 - accuracy: 0.2514 - val_loss: 1.8153 - val_accuracy: 0.2507 - lr: 1.0000e-05\n",
      "Epoch 16/20\n",
      "717/717 [==============================] - 556s 776ms/step - loss: 1.8158 - accuracy: 0.2513 - val_loss: 1.8146 - val_accuracy: 0.2504 - lr: 1.0000e-05\n",
      "Epoch 17/20\n",
      "717/717 [==============================] - 568s 792ms/step - loss: 1.8161 - accuracy: 0.2512 - val_loss: 1.8132 - val_accuracy: 0.2525 - lr: 1.0000e-06\n",
      "Epoch 18/20\n",
      "717/717 [==============================] - 501s 699ms/step - loss: 1.8148 - accuracy: 0.2513 - val_loss: 1.8150 - val_accuracy: 0.2511 - lr: 1.0000e-06\n",
      "Epoch 19/20\n",
      "717/717 [==============================] - 472s 657ms/step - loss: 1.8148 - accuracy: 0.2513 - val_loss: 1.8147 - val_accuracy: 0.2518 - lr: 1.0000e-06\n",
      "Epoch 20/20\n",
      "717/717 [==============================] - 465s 649ms/step - loss: 1.8157 - accuracy: 0.2515 - val_loss: 1.8137 - val_accuracy: 0.2532 - lr: 1.0000e-06\n",
      "89/89 [==============================] - 50s 564ms/step - loss: 1.8110 - accuracy: 0.2532\n",
      "Test Loss: 1.8109976053237915\n",
      "Test Accuracy: 0.25316011905670166\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "\n",
    "# Initially, freeze all layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(preprocessing.Rescaling(1./255, input_shape=(96, 96, 3)))\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Set the paths\n",
    "train_dir = 'C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output/train'\n",
    "test_dir = 'C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output/test'\n",
    "validation_dir = 'C:/Users/rohit/Machine_Learning/Facial_Expression_Detector/Output/validation'\n",
    "\n",
    "image_size = (96, 96, 3)\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation only for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generators\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=image_size[:2], batch_size=batch_size, class_mode='categorical')\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size=image_size[:2], batch_size=batch_size, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=image_size[:2], batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3877ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
